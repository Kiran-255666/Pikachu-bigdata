#Task-1
#To connect to the beeline:

beeline -u jdbc:hive2://localhost:10000/pikachu -u cloudera

Beeline
—————

show databases;
#Warning : Execute the below command only if the database doesn't exist
create database pikachu;

#In Hive QL, you can use the SHOW DATABASES statement with the LIKE clause to filter databases based on a pattern. However, Hive uses * for wildcard matching . Here's the syntax:
show databases like 'p*'

#to use pikachu database, execute the following command
use pikachu;

#To list tables in your 'pikachu' database, execute the following,
show tables;


###Explore Ware house directory
Terminal(
————————
 # you can use hadoop fs command or hdfs dfs command to explore the warehouse directory
hadoop fs  -ls /user/hive/warehouse

hadoop fs -ls /user/hive/warehouse/pikachu.db

hadoop fs -ls /user/hive/warehouse/pikachu.db/customers

hadoop fs -cat /user/hive/warehouse/pikachu.db/customers/000000_0

# In the above directory you can see the customers data that is stored in our hive warehouse directory



#You might have noticed that the default database doesn't have corresponding directoy, because all in default directory placed at the top direcotry /user/hive/warehouse
Beeline
————
# lets create a tabel named 'test'
use default;

create table test(id int, name string);

show tables;




Terminal
—————————
# Explore the warehouse
hadoop fs -ls /user/hive/warehouse/

hadoop fs -ls /user/hive/warehouse/test

What did you observe?


#Now lets try to insert data into 'test' table using the following statement

insert into test values (1234, 'Kiran');
# Now explore the directory again.
hadoop fs -cat /user/hive/warehouse/test/*


Beeline
—————
#Now, lets try to drop table 'test' by executing the following statement
drop table test;

#If you explore test directory in our terminal, you might have oberved that test directory is no longer present in our warehourse directory

##Task-2
Terminal
————————

#Download the products.csv dataset by executing the command in the Terminal 
wget "https://github.com/Kiran-255666/Pikachu-bigdata/raw/main/products.csv"

#To check the downloaded file, type 'ls' in the terminal, you will be able to see the downloaded products.csv file.
ls

#Let's create a directory named 'data' in our HDFS
hadoop fs -mkdir /data

#store the product.csv to '/data/'. you can copy from your Local file system to data diretory in HDFS.
hadoop fs -copyFromLocal products.csv /data/ 

#Now, Lets check whether the file is copied.
hadoop fs -ls /data/
#If you have successfully verified that your file has been copied, Continue....


Connect to Beeline:
beeline -u jdbc:hive2://localhost:10000/default -n cloudera
Beeline
————
# We will now create an eternal table products

#external- data is not managed by Hive, data for this table exists in HDFS directory you specifed.

create external table if not exists products (
id string,
title string,
cost float
)
comment "Table to store product information sold in stores"
location '/data/';

#If you run the following query,
select * from products;
(This is not what we wanted)
#All the products that we specified in csv file is available via Hive, As Hive is schema-on-read, the schema you defined for this table, hive will try to impose the Schema that you referrenced in HDFS.
#(This is not what we wanted), We got all the data in products.id column, and we have null values in other columns. 

Terminal
————————
#Lets explore whether our directory created in hive warehouse directoy.
hadoop fs -ls /user/hive/warehouse/pikachu.db
#(There should be nothing here)#because its external table

#The metadata for the external table is in metastore, and data is in '/data/'
hadoop fs -ls /data/
#(The data is here)

drop table products;

hadoop fs -ls /data/
#(The data is still here)# table no longer exist in database, its schema is also deleted, but the data is in hadoop fs -ls /data/


Beeline
————————
#Now, lets create the same table 'products' and load data correctly.

create external table if not exists products (
id string,
title string,
cost float
)
comment "Table to store product information sold in stores"
row format delimited fields terminated by ','
stored as textfile #here you can specify what kind of file that is being read but this is optional.
location '/data/';
#Row format delimited fields terminated by ',' Hive considers that each field is terminated by ','
select * from products;
(This is now how we want it)

#you can figure out whether the table are managed or external table, using the following commands.
describe formatted customers;
(Data should be in the warehouse, table type should be managed_table)

describe formatted products;
(Data should be in the HDFS location where we've placed it, table type should be external_table)

#Lets explore more ways to create tables.

#We use same pikachu database
Use pikachu;
#Lets create a table 'fresh_prouducts" using like keyword.Like keyword is used when you want to create a table similar to the table that you want(it should exist in database).Here we create fresh_products table with same column names and column datatypes like products.

create table if not exists fresh_products like products;

show tables;

describe fresh_products;

#you can rename a table using alter table keyword.Lets rename fresh_products to freshproducts.

alter table fresh_products 
rename to freshproducts;

show tables;

# you can also add columns to existing table using 'alter table' keyword.Lets add expiry_data column of type date along with the comment.

alter table freshproducts add columns (
expiry_date date 
comment "Expiry date of fresh produce"
);

describe freshproducts;

#you can also swap the columns using 'alter table' and at the same time you can also specify the datatype of column that you will be swapping.
alter table freshproducts 
change column id id string
after title;

#Now lets swap the columns of products table, and observe.
alter table products
change column id id string
after title;

(Note that the data does not change)


#We know that temporary tables we created in a hive session will get deleted whenthe session ends.
#Now, we will create a tempory table

create temporary table test_customers 
like customers;

show tables;

describe test_customers;
#Let's insert data into 'test_customers' table

insert into test_customers values (
9999,
"Jill",
"MN"
);

#Lets query data from test_customers table
select * from test_customers;
#Quit the hive session
!q

Terminal
————————
#connect to beeline


Beeline
————————

show tables;
#Now you might have obeserved that test_customers is no longer exists.

#Similarly lets create  a temperoroy table' test' just like orders table.
create temporary table test like orders;

describe test;

!q



Beeline
————————
describe test;
#Now you might have obeserved test is no longer exists.

#Download the freshproducts.csv dataset by executing the command in the Terminal 
wget "https://github.com/Kiran-255666/Pikachu-bigdata/raw/main/freshproducts.csv"

#Lets expore the freshproducts table
describe freshproducts;

#Lets swap the column title and id.
alter table freshproducts
change column title title string
after id;

#Now we will load data from freshproducts.csv, which in our local file system. Local keyword is used when you are trying to load data from local.
load data local inpath 'freshproducts.csv'
into table freshproducts; 
(This is from a local path and not HDFS)

select * from freshproducts;
(Note the null padding where we do not have information)


Terminal
————————

hadoop fs -ls /user/hive/warehouse/pikachu.db

hadoop fs -ls /user/hive/warehouse/pikachu.db/freshproducts

hadoop fs -cat /user/hive/warehouse/pikachu.db/freshproducts/*

(The CSV file has been moved into the warehouse directory)

(From hdfs)

hadoop fs -copyFromLocal freshproducts.csv /data/



Beeline
————————

Now, lets load the data from HDFS(This is from a HDFS path)
load data inpath '/data/freshproducts.csv' into table freshproducts; 

select * from freshproducts;

(Notice the data is appended, and both files are present in the warehouse directory)


Terminal
————————

hadoop fs -ls /user/hive/warehouse/freshproducts/

hadoop fs -cat /user/hive/warehouse/freshproducts/*

(Both files are present in the warehouse directory)

hadoop fs -ls /data/
(But the original file in Hadoop has moved in this case)


Beeline
————————

load data local inpath 'freshproducts.csv'
overwrite into table freshproducts; 

select * from freshproducts;


insert into table products
select id, title, cost from freshproducts;

select * from products;

insert overwrite table products
select id, title, cost from freshproducts;

select * from freshproducts;

alter table products
change column title title string
after id;

create table product_name (id string, name string);

create table product_cost (id string, cost float);

from products
insert overwrite table product_name
select id, title
insert into table product_cost
select id, cost;

select * from product_name;

select * from product_cost;

truncate table freshproducts;

select * from freshproducts;



























